import pandas as pd
import torch
import torch.nn as nn
from utils import metrics
import utils
import torchvision
import os

# def inverse_data_transform(X):
#     return torch.clamp((X + 1.0) / 2.0, 0.0, 1.0)
#     # return (X + 1.0) / 2.0

class DiffusiveRestoration:
    def __init__(self, diffusion, args, config):
        super(DiffusiveRestoration, self).__init__()
        self.args = args
        self.config = config
        self.diffusion = diffusion

        if os.path.isfile(args.resume):
            self.diffusion.load_ddm_ckpt(args.resume, ema=True)
            print('grid_r:', self.args.grid_r)
            self.diffusion.model.eval()
        else:
            print('Pre-trained diffusion model path is missing!')

    # def restore(self, val_loader, pred_model, r=None):
    def restore(self, val_loader, r=None):
        # 单张图
        image_folder = os.path.join(self.args.image_folder, self.config.training.name, self.config.training.version, 'epoch_700') # results/images/SAR/Synthesis/v3
        # image_folder = "/home/jbwei/190/Pke/test_syn_sar/sar_syn_output"
        # image_folder = "/home/jbwei/Pke/SARDiffusion/scratch/ckpts/"
        # image_folder = "/home/jbwei/190/Pke/SARDiffusion/results/images/SAR/Synthesis_no_log_SAR/sar_syn_x0Y_3090"
        # 一组图
        # image_folder = "/home/jbwei/190/ybma/code/data/SAR/despeckled/train/VH/" # results/images/SAR/Synthesis/v3
        with torch.no_grad():
            # for i, (x, y, lambda_) in enumerate(val_loader):
            for i, (x, y) in enumerate(val_loader):
                y = y[-1]
                print(f"starting processing from image {y}")
                print(x.shape)
                # lambda_ = lambda_.numpy()
                # print(lambda_)
                x = x.flatten(start_dim=0, end_dim=1) if x.ndim == 5 else x
                # Complex SAR
                x_cond = x[:, :4, :, :].to(self.diffusion.device)
                # GRD SAR
                # x_cond = x[:, :2, :, :].to(self.diffusion.device)
                # x_cond = utils.process(x_cond).to(self.diffusion.device)
                # xt = x[:, 4:, :, :].to(self.diffusion.device)
                x_output = self.diffusive_restoration(x_cond, x, r=r)
                # x_output = inverse_data_transform(x_output) # torch.Size([1,1,img_size,img_size])
                # utils.logging.save_image_v2(x_output,lambda_, os.path.join(image_folder, f"{y}_ddpm_x0Y_3090_best.tif")) # png->tif
                utils.logging.save_image(x_output, os.path.join(image_folder, f"{y}_ddpm_epoch_best.tif")) # png->tif

    def diffusive_restoration(self, x_cond, x, r=None):
        p_size = self.config.data.image_size
        h_list, w_list = self.overlapping_grid_indices(x_cond, output_size=p_size, r=r)
        corners = [(i, j) for i in h_list for j in w_list]
        # SLC
        x = torch.randn(x[:, 4:, :, :].size(), device=self.diffusion.device)
        # GRD
        # x = torch.randn(x[:, 2:, :, :].size(), device=self.diffusion.device)
        # x = utils.generalized_gamma(x_cond).to(self.diffusion.device)
        x_output = self.diffusion.sample_image(x_cond, x, patch_locs=corners, patch_size=p_size)
        # x_output = self.diffusion.sample_image(x_cond, x, patch_locs=None, patch_size=p_size)
        return x_output

    def overlapping_grid_indices(self, x_cond, output_size, r=None):
        _, c, h, w = x_cond.shape
        r = 16 if r is None else r
        h_list = [i for i in range(0, h - output_size + 1, r)] # [0, 16, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512, 528, 544, 560, 576]
        w_list = [i for i in range(0, w - output_size + 1, r)] # [0, 16, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512, 528, 544, 560, 576]
        return h_list, w_list